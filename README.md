# üöÄ PDF Analyzer con LLM Local/Online

Analiza art√≠culos acad√©micos en PDF y responde cuatro preguntas binarias (Yes/No) justificadas con **citas textuales** extra√≠das del documento.

## üì¶ Caracter√≠sticas
- Vista previa de la primera p√°gina del PDF.  
- Extracci√≥n de texto por `PyPDFium2Loader` y fallback por **OCR** (`pytesseract`) si no hay texto.  
- Chunking autom√°tico (`RecursiveCharacterTextSplitter`).  
- Embeddings con `sentence-transformers/all-MiniLM-L6-v2` y b√∫squeda sem√°ntica usando `FAISS`.  
- Soporta LLM local (**Ollama**) o LLMs online (**OpenAI**, **Anthropic**).  
- Prompt dise√±ado para responder 4 preguntas acad√©micas con formato:  
  `Yes/No ‚Äì "Cita exacta" ‚Äì Razonamiento breve`.

## üõ† Requisitos e instalaci√≥n
Instala dependencias Python:
```bash
pip install streamlit pymupdf pytesseract pdf2image langchain langchain-community langchain-openai langchain-anthropic sentence-transformers faiss-cpu
```

Dependencias del sistema:
- **Tesseract OCR**
  - Ubuntu/Debian: `sudo apt-get install tesseract-ocr`
- **Poppler** (requerido por `pdf2image`)
  - Ubuntu/Debian: `sudo apt-get install poppler-utils`
- **Ollama** (opcional, para LLM local): sigue instrucciones en https://ollama.ai

## ‚ñ∂Ô∏è C√≥mo ejecutar
1. Lanza la app:
```bash
streamlit run app.py
```
2. En la barra lateral:
   - Elige proveedor del LLM: `Ollama (Local)`, `OpenAI (Online)` o `Anthropic (Online)`.
   - Si eliges un proveedor online, pega tu **API Key**.
   - Ajusta el nombre del modelo si lo deseas.
3. Sube un PDF (`.pdf`). Espera la vista previa de la primera p√°gina.
4. Presiona **Generar respuesta** para recibir las 4 respuestas con citas y justificaci√≥n.

## üîÅ Flujo interno (resumen)
1. Guardado temporal del PDF (`temp.pdf`).  
2. Extracci√≥n con `PyPDFium2Loader`. Si no hay texto, OCR p√°gina a p√°gina con `pytesseract`.  
3. Divisi√≥n en chunks (500 tokens aprox., overlap 50).  
4. Creaci√≥n de embeddings (`HuggingFaceEmbeddings`) y FAISS vectorstore.  
5. Creaci√≥n de un `retriever` y construcci√≥n de una cadena `RetrievalQA` con un prompt especializado.  
6. LLM responde y la app muestra la salida junto con el tiempo transcurrido.

## üß≠ Formato esperado de la respuesta
Cada pregunta debe devolver:
```
Question: [Pregunta]
Answer format: Yes/No ‚Äì "Cita exacta del art√≠culo" ‚Äì [Razonamiento breve]
```
Ejemplo:
```
Question: Is the study empirical?
Answer format: Yes ‚Äì "We performed a survey of 300 participants..." ‚Äì El texto describe un muestreo y an√°lisis emp√≠rico.
```

## ‚ö†Ô∏è Consejos y notas
- Cambia `pytesseract.image_to_string(..., lang="spa")` si el PDF est√° en otro idioma.  
- OCR en PDFs largos puede tardar ‚Äî ten paciencia.  
- La calidad de la respuesta depende de la extracci√≥n de texto y del modelo LLM elegido.  
- Si usas OpenAI/Anthropic, aseg√∫rate de que tu API Key tenga permisos y saldo suficiente.

